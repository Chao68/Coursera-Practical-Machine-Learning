---
title: "Practical Machine Learning - Prediction Assignment"
author: "Szu-Fu Chao"
date: "6/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### [Data]
The train data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv 
&nbsp;

The test data are available here:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
&nbsp;

More information regarding the data background are available here:  
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har 

```{r, message=F}
library(tidyverse)
library(naniar)
library(caret)
```

### **Goal**  
The goal of the assignment is to use the measurements collected from the accelerometers to predict how well participants perform the weight lifting excecise. 

### **Data Exploring and Preprocessing**
```{r, message=F}
train <- read.csv("pml-training.csv")
dim(train)
set.seed(100)
inTrain <- createDataPartition(train$classe, p = 0.75, list = F)
training <- train[inTrain, ]
testing <- train[-inTrain, ]
dim(training)
dim(testing)
```
There were a total of 160 variables and 19,622 observations in the train data. We first split the data into two subsets: training set (160 variables and 14,718 obs) and testing set (160 variables and 4,904 obs). We wanted to use the training set to train our predictive model, and then apply the model to the testing set.  

```{r, message=F}
# drop the first 7 non-critical variables from both the training and testing sets
training <- training[-c(1:7)] 
testing <- testing[-c(1:7)] 

# make "classe" as factor in both the training set and testing sets
training$classe <- as.factor(training$classe)
testing$classe <- as.factor(testing$classe) 

# recode undesired "#DIV/0!" to NA in both the training set and testing sets
training[training =="#DIV/0!"] <- NA
testing[testing =="#DIV/0!"] <- NA 

# make all predictors as numeric in both the training set and testing sets
for (i in 1:152) {if (class(training[,i])=="character") {training[,i] <- as.numeric(training[,i])}}
for (i in 1:152) {if (class(testing[,i])=="character") {testing[,i] <- as.numeric(testing[,i])}}
```
After some minor but necessary modifications, we further explored the missing information since prediction algorithm was not built to handle missing in most cases. 

```{r}
# examine missing 
vis_miss(training[-153], warn_large_data = F) # missing pattern
```
&nbsp;

The plot above provided a visualization of the amount of missing data. It showed that the variables with missing all had extremely high missing rate (close to all missing). 

```{r}
# identify variables with at least 95% missing rate
toExclude <- names(training)[-153][sapply(names(training)[-153], function(x) sum(is.na(training[x]))/nrow(training)>0.95)] 

# drop the variables with severe missing
training <- training[!names(training) %in% toExclude] 
testing <- testing[!names(testing) %in% toExclude] 

# use nearZeroVar to further check if remaining predictors have near zero variance
length(nearZeroVar(training[-153]))

dim(training)
dim(testing)
```
Given the circumstance, it might not be appropriate to use imputation to recover data therefore these variables were dropped. For the remaining variables, nearZeroVar function was used to detect predictors with near zero variance. None was identified. As a result, a total of 53 variables (52 predictors and 1 outcome variable) were included for modeling. 

### **Model Training**
Tree-based algorithms - "Random Forest" and "GBM" - were used to train the model so that we didn't have to worry about the underlying assumption regarding a certain data distribution which a model-based algorithm would need. The two methods are widely used and often achieve good performance in prediction. 

#### *Random Forest*
```{r}
set.seed(201)
start_time <- Sys.time() # the begining of model training
mod_rf <- train(classe~., data=training, method="rf", trControl=trainControl(method = "cv"), number=3)
end_time <- Sys.time() # the end of model training
end_time - start_time # total processing time
confusionMatrix(predict(mod_rf, testing), testing$classe)
```

#### *GBM*
```{r}
set.seed(202)
start_time <- Sys.time() # the begining of model training
mod_gbm <- train(classe~., data=training, method="gbm", verbose = F)
end_time <- Sys.time() # the end of model training
end_time - start_time # total processing time
confusionMatrix(predict(mod_gbm, testing), testing$classe)
```

### **Modeling Conclusion**
The results showed that when applying the models to the testing set, the model using random forest provided a slightly better accuracy than the model using GBM. (RF accuracy = 0.9935 and GBM accuracy = 0.9655). Regarding the processing time, we observed that the random forest approach, with a 3-fold cross-validation implemented to fight for overfitting, completed a little faster than the GBM. (RF processting time ~ 23 min and GBM processing time ~ 25 min). Therefore, the model using random forest algorithm seemed to be a better choice for the data.

### **Prediction on the Test Data**
Lastly, we applied the random forest model to the test data to predict "classe" for the 20 observations.
```{r}
test <- read.csv("pml-testing.csv")
predict(mod_rf, test)
```


